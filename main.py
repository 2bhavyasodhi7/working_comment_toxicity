# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e_Zs3sgwZeWRhZF9r12x2B_bgEkSaIIB
"""

import streamlit as st
import numpy as np
import requests
import os
import tensorflow as tf

# Hugging Face or raw file direct URL (replace with yours if needed)
MODEL_URL = "https://huggingface.co/2bhavyasodhi7/comment_toxicity/blob/main/toxicity.h5"
MODEL_PATH = "toxicity.h5"

# Download and cache the model
@st.cache_resource
def download_model():
    if not os.path.exists(MODEL_PATH):
        with st.spinner("Downloading model... Please wait ‚è≥"):
            response = requests.get(MODEL_URL, stream=True)
            with open(MODEL_PATH, "wb") as file:
                for chunk in response.iter_content(chunk_size=8192):
                    file.write(chunk)
    return tf.keras.models.load_model(MODEL_PATH)

# Load model
model = download_model()

# Load vectorizer layer if saved as part of model
text_vectorizer = model.layers[0]

# Predict function
def predict_toxicity(comment):
    vectorized_text = text_vectorizer(np.array([comment]))
    prediction = model.predict(vectorized_text)[0]
    labels = ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]

    toxic_labels = [labels[i] for i, val in enumerate(prediction) if val > 0.5]
    return toxic_labels if toxic_labels else ["Clean / Non-toxic"]

# UI
st.title("üí¨ Toxic Comment Detector")
st.markdown("Enter a comment, and the model will classify if it is **toxic** or **non-toxic**.")

user_input = st.text_area("‚úçÔ∏è Type your comment here:")

if st.button("Analyze"):
    if user_input.strip():
        prediction = predict_toxicity(user_input)
        st.write("üö® **Toxic Categories:**", ", ".join(prediction))
    else:
        st.warning("Please enter a comment to analyze.")

# About
st.markdown("""
### ‚ÑπÔ∏è About the Model
This model classifies comments into six categories:
- Toxic
- Severe Toxic
- Obscene
- Threat
- Insult
- Identity Hate

It uses a CNN/RNN with a text vectorization layer trained on the [Jigsaw Toxic Comment Dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).
""")